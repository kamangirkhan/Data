{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamangirkhan/Data/blob/main/ArashNateghian_HW_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6cedab4f-ec12-497a-8e4c-29168e024e9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cedab4f-ec12-497a-8e4c-29168e024e9a",
        "outputId": "f030584b-be5a-4d42-f5c7-e16446729d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page title: Artificial intelligence - Wikipedia\n",
            "\n",
            "=== Content section (first 10 headings) ===\n",
            "Contents\n",
            "Goals\n",
            "Reasoning and problem-solving\n",
            "Knowledge representation\n",
            "Planning and decision-making\n",
            "Learning\n",
            "Natural language processing\n",
            "Perception\n",
            "Social intelligence\n",
            "General intelligence\n",
            "\n",
            "Saved 53 headings to content.txt\n",
            "\n",
            "=== See also (first 10 links) ===\n",
            "/wiki/Environmental_impacts_of_artificial_intelligence -> Environmental impacts of artificial intelligence\n",
            "/wiki/Content_moderation -> Content moderation\n",
            "/wiki/Explainable_AI -> Explainable AI\n",
            "/wiki/Algorithmic_transparency -> Algorithmic transparency\n",
            "/wiki/Right_to_explanation -> Right to explanation\n",
            "/wiki/Human-AI_interaction -> Human-AI interaction\n",
            "/wiki/Lists_of_open-source_artificial_intelligence_software -> Lists of open-source artificial intelligence software\n",
            "/wiki/Synthetic_intelligence -> Synthetic intelligence\n",
            "/wiki/Intelligent_agent -> Intelligent agent\n",
            "/wiki/Artificial_mind_(disambiguation) -> Artificial mind\n",
            "\n",
            "Saved 12 'See also' links to see_also.csv\n"
          ]
        }
      ],
      "source": [
        "import urllib.request, urllib.error\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def get_soup(url):\n",
        "\n",
        "    req = urllib.request.Request(\n",
        "        url,\n",
        "        headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    )\n",
        "    try:\n",
        "        with urllib.request.urlopen(req) as resp:\n",
        "            html = resp.read()\n",
        "        return BeautifulSoup(html, \"html.parser\")\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching page:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Part 1 – Tutorial-style scraping on AI Wikipedia page\n",
        "url_ai = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "soup_ai = get_soup(url_ai)\n",
        "\n",
        "if not soup_ai:\n",
        "    raise SystemExit(\"Could not load AI page.\")\n",
        "\n",
        "# --- Title ---\n",
        "title = soup_ai.title.get_text(strip=True) if soup_ai.title else \"NO TITLE\"\n",
        "print(\"Page title:\", title)\n",
        "\n",
        "# --- \"Content\": collect all section headings as our 'content list' ---\n",
        "headings = []\n",
        "for tag in soup_ai.find_all([\"h2\", \"h3\"]):\n",
        "    span = tag.find(\"span\", class_=\"mw-headline\")\n",
        "    text = span.get_text(strip=True) if span else tag.get_text(strip=True)\n",
        "    text = text.replace(\"[edit]\", \"\").strip()\n",
        "    if text:\n",
        "        headings.append(text)\n",
        "\n",
        "print(\"\\n=== Content section (first 10 headings) ===\")\n",
        "for h in headings[:10]:\n",
        "    print(h)\n",
        "\n",
        "with open(\"content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for h in headings:\n",
        "        f.write(h + \"\\n\")\n",
        "\n",
        "print(f\"\\nSaved {len(headings)} headings to content.txt\")\n",
        "\n",
        "\n",
        "\n",
        "see_also = []\n",
        "\n",
        "# 1) Collect all hatnotes that start with \"See also:\"\n",
        "for note in soup_ai.find_all(\"div\", class_=\"hatnote\"):\n",
        "    text = note.get_text(\" \", strip=True)\n",
        "    if \"see also\" in text.lower():\n",
        "        for a_tag in note.find_all(\"a\", href=True):\n",
        "            href = a_tag[\"href\"]\n",
        "            label = a_tag.get_text(strip=True)\n",
        "            if label:\n",
        "                see_also.append([href, label])\n",
        "\n",
        "# 2) collect a global \"See also\" section if it exists\n",
        "see_header = None\n",
        "for tag in soup_ai.find_all([\"h2\", \"h3\"]):\n",
        "    heading_text = tag.get_text(\" \", strip=True).lower()\n",
        "    if \"see also\" in heading_text:\n",
        "        see_header = tag\n",
        "        break\n",
        "\n",
        "if see_header:\n",
        "    for sib in see_header.find_next_siblings():\n",
        "        if sib.name in [\"h2\", \"h3\"]:\n",
        "            break  # next section, stop\n",
        "        if sib.name == \"ul\":\n",
        "            for li in sib.find_all(\"li\"):\n",
        "                a_tag = li.find(\"a\", href=True)\n",
        "                if not a_tag:\n",
        "                    continue\n",
        "                href = a_tag[\"href\"]\n",
        "                label = a_tag.get_text(strip=True)\n",
        "                if label:\n",
        "                    see_also.append([href, label])\n",
        "\n",
        "print(\"\\n=== See also (first 10 links) ===\")\n",
        "for href, label in see_also[:10]:\n",
        "    print(href, \"->\", label)\n",
        "\n",
        "# Save to CSV\n",
        "with open(\"see_also.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"href\", \"text\"])\n",
        "    writer.writerows(see_also)\n",
        "\n",
        "print(f\"\\nSaved {len(see_also)} 'See also' links to see_also.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc4af26-76a2-49a6-88f3-09babe69f115",
      "metadata": {
        "id": "1cc4af26-76a2-49a6-88f3-09babe69f115"
      },
      "source": [
        "**Web Scraping Learning Experience**\n",
        "\n",
        "In this project, I explored the fundamentals of web scraping using Python, urllib.request, and BeautifulSoup. My goal was to follow a tutorial that scraped the “Content” and “See also” sections from a Wikipedia page, and then apply and enhance this method on my own. Although the tutorial appeared straightforward, my actual experience differed significantly because the HTML layout of Wikipedia pages has changed over time. This forced me to move beyond copy-and-paste scripting and actually understand how web scraping works under dynamic, real-world conditions.\n",
        "\n",
        "**What I Learned**\n",
        "\n",
        "The most important thing I learned is that web scraping is not about memorizing code, it is about understanding structure. HTML changes, websites update their layout, and hard-coded patterns from tutorials often fail. I learned how to:\n",
        "\n",
        "Inspect live HTML and search for patterns (h2, h3, span.id, class=\"hatnote\", etc.).\n",
        "\n",
        "Use req = urllib.request.Request(...) with a User-Agent header to avoid 403 Forbidden errors.\n",
        "\n",
        "Parse webpages using BeautifulSoup and extract tags, text, and attributes.\n",
        "\n",
        "Handle unexpected or missing elements defensively (None checking, conditional extraction).\n",
        "\n",
        "Use multiple fallback patterns when a single selector doesn’t work.\n",
        "\n",
        "This project forced me to think like a scraper developer rather than a code copier.\n",
        "\n",
        "**Challenges I Encountered**\n",
        "\n",
        "I encountered several meaningful challenges:\n",
        "\n",
        "Wikipedia changed its HTML structure, so the tutorial’s code for \"See also\" no longer worked at all.\n",
        "I kept getting:\n",
        "“Saved 0 ‘See also’ links to see_also.csv.”\n",
        "\n",
        "The tutorial relied on div class=\"div-col\", which no longer exists on the page I scraped.\n",
        "\n",
        "The page had many hatnotes with small “See also:” hints inside sections, but did not have a properly structured bottom “See also” section like older pages.\n",
        "\n",
        "I had to debug why soup.find('div', class='div-col') returned None and why see_header.find_next_siblings() didn’t locate any lists.\n",
        "\n",
        "I also had to deal with relative URLs (/wiki/...) and later learned how to convert them into full URLs using urljoin.\n",
        "\n",
        "These failures were actually the best part of the learning process, because they pushed me to analyze HTML more carefully and redesign the extraction logic.\n",
        "\n",
        "**Enhancements I Made**\n",
        "\n",
        "I implemented several enhancements beyond the tutorial, all of which strengthened the scraper:\n",
        "\n",
        " Added a Realistic User-Agent Header\n",
        "\n",
        "Without this, Wikipedia returned a 403 Forbidden error.\n",
        "This makes the scraper more stable and more “browser-like.”\n",
        "\n",
        "\n",
        "**Overall Learning Experience**\n",
        "\n",
        "This assignment transformed my understanding of web scraping. I began by expecting to simply replicate the tutorial, but the failures forced me to analyze HTML deeply, think critically, and improve the scraper’s logic. I learned that web scraping is fundamentally a problem-solving activity, where the developer must adapt to imperfect or changing HTML structures.\n",
        "\n",
        "This experience also clarified the difference between:\n",
        "\n",
        "Theoretical examples that work in controlled tutorials\n",
        "\n",
        "Real-world pages that evolve, break older selectors, and require flexible strategies\n",
        "\n",
        "In the end, I created a significantly improved version of the code with modern, reliable extraction of both headings and “See also” links. The debugging process improved my HTML literacy, my BeautifulSoup skills, and my ability to design fallback logic.\n",
        "\n",
        "Overall, this was an extremely valuable and realistic introduction to web scraping."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}